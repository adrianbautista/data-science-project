{
 "metadata": {
  "name": "",
  "signature": "sha256:f9da99677fe96f1d76e501e72cc73361bbec4ba7c03cacd1e6247716996a90c8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import folium as fm\n",
      "import geopy\n",
      "\n",
      "from sklearn import linear_model, naive_bayes, feature_selection, metrics, tree\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tips_with_adjectives_1 = pd.read_pickle('./dumps/tips_with_adjectives.pkl')\n",
      "adj_dummies = pd.read_pickle('./dumps/adjective_dataframe.pkl')\n",
      "adj_df = pd.read_csv('./dumps/adjective_count_list.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(adj_dummies)\n",
      "print len(tips_with_adjectives)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15527\n",
        "15527\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adjective_list = list(adj_df[adj_df['count'] > 10]['word'])\n",
      "\n",
      "print \"Number of tips: \", len(tips_with_adjectives_1)\n",
      "print \"Number of adjectives: \", len(adj_df)\n",
      "print \"Number of significant adjectives (appears in more than 10 tips): \", len(adjective_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of tips:  15527\n",
        "Number of adjectives:  1838\n",
        "Number of significant adjectives (appears in more than 10 tips):  255\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tips_with_adjectives['address'] = tips_with_adjectives.apply(lambda x: \"{0} {1} {2}, {3}, New York, NY\".format(x['BUILDING'].strip(), x['STREET'].strip(), int(x['ZIPCODE']), x['BORO']), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "latlong_df = pd.read_pickle('./dumps/with_lat_long.pkl')[['foursquare_id', 'lat_long']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tips_adj_df = tips_with_adjectives_1.join(adj_dummies)\n",
      "tips_adj_df.drop_duplicates(['foursquare_id', 'description'], inplace=True)\n",
      "tips_adj_df = tips_adj_df.merge(latlong_df, on='foursquare_id', how='left')\n",
      "tips_adj_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>foursquare_id</th>\n",
        "      <th>description</th>\n",
        "      <th>created_at</th>\n",
        "      <th>DBA</th>\n",
        "      <th>BORO</th>\n",
        "      <th>BUILDING</th>\n",
        "      <th>STREET</th>\n",
        "      <th>ZIPCODE</th>\n",
        "      <th>PHONE</th>\n",
        "      <th>CUISINE DESCRIPTION</th>\n",
        "      <th>...</th>\n",
        "      <th>whole</th>\n",
        "      <th>wonderful</th>\n",
        "      <th>worth</th>\n",
        "      <th>wrong</th>\n",
        "      <th>yellow</th>\n",
        "      <th>youll</th>\n",
        "      <th>young</th>\n",
        "      <th>yous</th>\n",
        "      <th>yummy</th>\n",
        "      <th>lat_long</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 4e823597775b368bbbffb549</td>\n",
        "      <td> the breakfast here is good the staff needs som...</td>\n",
        "      <td> 1359463129</td>\n",
        "      <td>                        WENDY'S</td>\n",
        "      <td>  BROOKLYN</td>\n",
        "      <td>        469</td>\n",
        "      <td>                                   FLATBUSH AVENUE</td>\n",
        "      <td> 11225</td>\n",
        "      <td> 7182875005</td>\n",
        "      <td>   Hamburgers</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> (40.6630011878, -73.9618386697)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 4a7ef04bf964a52055f21fe3</td>\n",
        "      <td> eat brunch here for very reasonably priced del...</td>\n",
        "      <td> 1312045610</td>\n",
        "      <td> DJ REYNOLDS PUB AND RESTAURANT</td>\n",
        "      <td> MANHATTAN</td>\n",
        "      <td> 351       </td>\n",
        "      <td> WEST   57 STREET                              ...</td>\n",
        "      <td> 10019</td>\n",
        "      <td> 2122452912</td>\n",
        "      <td>        Irish</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>  (40.7676882095, -73.985037451)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 4ac68680f964a520d8b420e3</td>\n",
        "      <td> provided my wife and i with the best wedding e...</td>\n",
        "      <td> 1351084714</td>\n",
        "      <td>                RIVIERA CATERER</td>\n",
        "      <td>  BROOKLYN</td>\n",
        "      <td>       2780</td>\n",
        "      <td>                                  STILLWELL AVENUE</td>\n",
        "      <td> 11224</td>\n",
        "      <td> 7183723031</td>\n",
        "      <td>    American </td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> (40.5796712021, -73.9822941487)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4c12d4b882a3c9b6ac7bfaf8</td>\n",
        "      <td>                           when u sleep dont sleep</td>\n",
        "      <td> 1306075709</td>\n",
        "      <td>        BRUNOS ON THE BOULEVARD</td>\n",
        "      <td>    QUEENS</td>\n",
        "      <td> 8825      </td>\n",
        "      <td> ASTORIA BOULEVARD                             ...</td>\n",
        "      <td> 11369</td>\n",
        "      <td> 7183350505</td>\n",
        "      <td>    American </td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> (40.7641288111, -73.8808250427)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 4bdc87a7afe8c9b645dc4f85</td>\n",
        "      <td> get the corn chowder over every other soup awe...</td>\n",
        "      <td> 1327798534</td>\n",
        "      <td>             WILKEN'S FINE FOOD</td>\n",
        "      <td>  BROOKLYN</td>\n",
        "      <td> 7114      </td>\n",
        "      <td>                  AVENUE U                        </td>\n",
        "      <td> 11234</td>\n",
        "      <td> 7184443838</td>\n",
        "      <td> Delicatessen</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> (40.6200724816, -73.9068323117)</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 290 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "              foursquare_id  \\\n",
        "0  4e823597775b368bbbffb549   \n",
        "1  4a7ef04bf964a52055f21fe3   \n",
        "2  4ac68680f964a520d8b420e3   \n",
        "3  4c12d4b882a3c9b6ac7bfaf8   \n",
        "4  4bdc87a7afe8c9b645dc4f85   \n",
        "\n",
        "                                         description  created_at  \\\n",
        "0  the breakfast here is good the staff needs som...  1359463129   \n",
        "1  eat brunch here for very reasonably priced del...  1312045610   \n",
        "2  provided my wife and i with the best wedding e...  1351084714   \n",
        "3                            when u sleep dont sleep  1306075709   \n",
        "4  get the corn chowder over every other soup awe...  1327798534   \n",
        "\n",
        "                              DBA       BORO    BUILDING  \\\n",
        "0                         WENDY'S   BROOKLYN         469   \n",
        "1  DJ REYNOLDS PUB AND RESTAURANT  MANHATTAN  351          \n",
        "2                 RIVIERA CATERER   BROOKLYN        2780   \n",
        "3         BRUNOS ON THE BOULEVARD     QUEENS  8825         \n",
        "4              WILKEN'S FINE FOOD   BROOKLYN  7114         \n",
        "\n",
        "                                              STREET  ZIPCODE       PHONE  \\\n",
        "0                                    FLATBUSH AVENUE    11225  7182875005   \n",
        "1  WEST   57 STREET                              ...    10019  2122452912   \n",
        "2                                   STILLWELL AVENUE    11224  7183723031   \n",
        "3  ASTORIA BOULEVARD                             ...    11369  7183350505   \n",
        "4                   AVENUE U                            11234  7184443838   \n",
        "\n",
        "  CUISINE DESCRIPTION         ...         whole wonderful worth wrong yellow  \\\n",
        "0          Hamburgers         ...             0         0     0     0      0   \n",
        "1               Irish         ...             0         0     0     0      0   \n",
        "2           American          ...             0         0     0     0      0   \n",
        "3           American          ...             0         0     0     0      0   \n",
        "4        Delicatessen         ...             0         0     0     0      0   \n",
        "\n",
        "   youll young yous yummy                         lat_long  \n",
        "0      0     0    0     0  (40.6630011878, -73.9618386697)  \n",
        "1      0     0    0     0   (40.7676882095, -73.985037451)  \n",
        "2      0     0    0     0  (40.5796712021, -73.9822941487)  \n",
        "3      0     0    0     0  (40.7641288111, -73.8808250427)  \n",
        "4      0     0    0     0  (40.6200724816, -73.9068323117)  \n",
        "\n",
        "[5 rows x 290 columns]"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# desired_columns = [\n",
      "#     'foursquare_id',\n",
      "#     'DBA',\n",
      "#     'description', \n",
      "#     'tip_words', \n",
      "#     'tip_adjs', \n",
      "#     'adj_string', \n",
      "#     'foursquare_rating',\n",
      "#     'foursquare_num_of_users',\n",
      "#     'foursquare_price_tier',\n",
      "#     'grade_A', \n",
      "#     'grade_C',\n",
      "#     'GRADE',\n",
      "#     'lat_long'\n",
      "# ]\n",
      "\n",
      "# tips_df = tips_with_adjectives[desired_columns]\n",
      "# len(tips_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fit_and_score(model, x_features, y_targets, columns, model_type):\n",
      "    model = model.fit(x_features, y_targets)\n",
      "    score = model.score(x_features, y_targets)\n",
      "    y_pred = model.predict(x_features)\n",
      "    auc = metrics.roc_auc_score(y_targets, y_pred)\n",
      "    \n",
      "    p_values = feature_selection.f_classif(x_features, y_targets)\n",
      "    \n",
      "    if model_type == 'naive-bayes' or model_type == 'logistic':\n",
      "        coef_list = [np.exp(round(x, 4)) for x in model.coef_[0]]\n",
      "    elif model_type == 'linear':\n",
      "        coef_list = [round(x, 4) for x in model.coef_]\n",
      "        \n",
      "    \n",
      "    df_dict = {'adjective': columns, 'p-value': p_values[0], 'coef': coef_list}\n",
      "    model_df = pd.DataFrame(df_dict)\n",
      "    model_df.sort(['p-value', 'coef'], ascending=[1,0], inplace=True)\n",
      "    \n",
      "    print 'MODEL: ', model\n",
      "    print 'SCORE: ', score\n",
      "    print 'AUC: ', auc\n",
      "    print '\\n'\n",
      "    \n",
      "    print 'TOP PREDICTORS (p-value < 0.05):'\n",
      "    print model_df[model_df['p-value'] <= 0.05]\n",
      "    print '\\n'\n",
      "    \n",
      "#     return model_df\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for index, column in enumerate(tips_adj_df.columns.values):\n",
      "    print column, index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "foursquare_id 0\n",
        "description 1\n",
        "created_at 2\n",
        "DBA 3\n",
        "BORO 4\n",
        "BUILDING 5\n",
        "STREET 6\n",
        "ZIPCODE 7\n",
        "PHONE 8\n",
        "CUISINE DESCRIPTION 9\n",
        "INSPECTION DATE 10\n",
        "ACTION 11\n",
        "VIOLATION CODE 12\n",
        "VIOLATION DESCRIPTION 13\n",
        "CRITICAL FLAG 14\n",
        "SCORE 15\n",
        "GRADE 16\n",
        "GRADE DATE 17\n",
        "RECORD DATE 18\n",
        "INSPECTION TYPE 19\n",
        "foursquare_name 20\n",
        "foursquare_rating 21\n",
        "foursquare_category 22\n",
        "foursquare_num_of_users 23\n",
        "foursquare_price_tier 24\n",
        "grade_A 25\n",
        "grade_B 26\n",
        "grade_C 27\n",
        "grade_Not Yet Graded 28\n",
        "grade_Z 29\n",
        "tip_words 30\n",
        "tip_adjs 31\n",
        "adjective_count 32\n",
        "adj_string 33\n",
        "20500daily 34\n",
        "accept 35\n",
        "actual 36\n",
        "addictive 37\n",
        "adventurous 38\n",
        "affordable 39\n",
        "american 40\n",
        "asian 41\n",
        "attentive 42\n",
        "authentic 43\n",
        "available 44\n",
        "average 45\n",
        "awful 46\n",
        "bad 47\n",
        "baked 48\n",
        "basic 49\n",
        "bean 50\n",
        "beat 51\n",
        "beautiful 52\n",
        "big 53\n",
        "black 54\n",
        "blue 55\n",
        "breakfast 56\n",
        "bubble 57\n",
        "bushy 58\n",
        "busy 59\n",
        "cant 60\n",
        "careful 61\n",
        "caribbean 62\n",
        "casual 63\n",
        "central 64\n",
        "certain 65\n",
        "cheese 66\n",
        "cheesy 67\n",
        "chicken 68\n",
        "chinese 69\n",
        "chocolate 70\n",
        "classic 71\n",
        "clean 72\n",
        "clear 73\n",
        "close 74\n",
        "cold 75\n",
        "comfortable 76\n",
        "complimentary 77\n",
        "congested 78\n",
        "cool 79\n",
        "cozy 80\n",
        "crazy 81\n",
        "creative 82\n",
        "cute 83\n",
        "daily 84\n",
        "decent 85\n",
        "delicious 86\n",
        "delish 87\n",
        "different 88\n",
        "dirty 89\n",
        "dish 90\n",
        "dive 91\n",
        "dont 92\n",
        "double 93\n",
        "drive 94\n",
        "dry 95\n",
        "early 96\n",
        "earth 97\n",
        "east 98\n",
        "easy 99\n",
        "eat 100\n",
        "efficient 101\n",
        "eggplant 102\n",
        "empty 103\n",
        "english 104\n",
        "enough 105\n",
        "entire 106\n",
        "epic 107\n",
        "excellent 108\n",
        "expensive 109\n",
        "extra 110\n",
        "fabulous 111\n",
        "famous 112\n",
        "fantastic 113\n",
        "fast 114\n",
        "fat 115\n",
        "favorite 116\n",
        "few 117\n",
        "first 118\n",
        "fish 119\n",
        "flat 120\n",
        "flavorful 121\n",
        "free 122\n",
        "french 123\n",
        "fresh 124\n",
        "fried 125\n",
        "friendly 126\n",
        "full 127\n",
        "funny 128\n",
        "garlic 129\n",
        "general 130\n",
        "generous 131\n",
        "giant 132\n",
        "ginormous 133\n",
        "give 134\n",
        "goat 135\n",
        "good 136\n",
        "gorgeous 137\n",
        "greasy 138\n",
        "great 139\n",
        "green 140\n",
        "grilled 141\n",
        "happy 142\n",
        "hard 143\n",
        "healthy 144\n",
        "heavy 145\n",
        "helpful 146\n",
        "high 147\n",
        "horrible 148\n",
        "hot 149\n",
        "huge 150\n",
        "iconic 151\n",
        "incredible 152\n",
        "indian 153\n",
        "inexpensive 154\n",
        "interesting 155\n",
        "irish 156\n",
        "italian 157\n",
        "ive 158\n",
        "japanese 159\n",
        "key 160\n",
        "kid 161\n",
        "kind 162\n",
        "korean 163\n",
        "large 164\n",
        "last 165\n",
        "late 166\n",
        "later 167\n",
        "light 168\n",
        "little 169\n",
        "live 170\n",
        "local 171\n",
        "long 172\n",
        "low 173\n",
        "magic 174\n",
        "main 175\n",
        "major 176\n",
        "many 177\n",
        "massive 178\n",
        "mean 179\n",
        "mediocre 180\n",
        "mexican 181\n",
        "modern 182\n",
        "much 183\n",
        "music 184\n",
        "natural 185\n",
        "nearby 186\n",
        "new 187\n",
        "next 188\n",
        "nice 189\n",
        "normal 190\n",
        "old 191\n",
        "olive 192\n",
        "only 193\n",
        "open 194\n",
        "organic 195\n",
        "original 196\n",
        "other 197\n",
        "outstanding 198\n",
        "overall 199\n",
        "own 200\n",
        "past 201\n",
        "perfect 202\n",
        "personal 203\n",
        "phenomenal 204\n",
        "pic 205\n",
        "pleasant 206\n",
        "polish 207\n",
        "poor 208\n",
        "popular 209\n",
        "priceless 210\n",
        "private 211\n",
        "public 212\n",
        "quick 213\n",
        "quiet 214\n",
        "ready 215\n",
        "real 216\n",
        "reasonable 217\n",
        "red 218\n",
        "regular 219\n",
        "ridiculous 220\n",
        "rosemary 221\n",
        "royal 222\n",
        "rude 223\n",
        "russian 224\n",
        "sad 225\n",
        "salad 226\n",
        "saltfish 227\n",
        "same 228\n",
        "second 229\n",
        "separate 230\n",
        "serious 231\n",
        "several 232\n",
        "short 233\n",
        "sicilian 234\n",
        "similar 235\n",
        "simple 236\n",
        "sized 237\n",
        "slow 238\n",
        "small 239\n",
        "smile 240\n",
        "social 241\n",
        "soft 242\n",
        "solid 243\n",
        "spacious 244\n",
        "spanish 245\n",
        "special 246\n",
        "spectacular 247\n",
        "standard 248\n",
        "steak 249\n",
        "stellar 250\n",
        "straight 251\n",
        "strong 252\n",
        "such 253\n",
        "super 254\n",
        "superior 255\n",
        "sure 256\n",
        "sweet 257\n",
        "swiss 258\n",
        "table 259\n",
        "tasty 260\n",
        "terrible 261\n",
        "terrific 262\n",
        "thin 263\n",
        "tiny 264\n",
        "top 265\n",
        "traditional 266\n",
        "true 267\n",
        "turkish 268\n",
        "typical 269\n",
        "unbeatable 270\n",
        "unbelievable 271\n",
        "unique 272\n",
        "usual 273\n",
        "vegetable 274\n",
        "vegetarian 275\n",
        "want 276\n",
        "weak 277\n",
        "weird 278\n",
        "white 279\n",
        "whole 280\n",
        "wonderful 281\n",
        "worth 282\n",
        "wrong 283\n",
        "yellow 284\n",
        "youll 285\n",
        "young 286\n",
        "yous 287\n",
        "yummy 288\n",
        "lat_long 289\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ADJECTIVE COLUMNS ARE 31 till second to last column\n",
      "# based on adjectives included in tip descriptions\n",
      "X_adjs = tips_adj_df.ix[:, 34:-1]\n",
      "\n",
      "# based on ratings, number of users, and price tier\n",
      "X_foursquare_info = tips_adj_df[['foursquare_rating', 'foursquare_num_of_users', 'foursquare_price_tier']].dropna(axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Predicting Grade \"A\" restaurants"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = tips_adj_df['grade_A']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = train_test_split(X_adjs, y, random_state=8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_multi_nb = naive_bayes.MultinomialNB()\n",
      "\n",
      "print 'Using training set'\n",
      "fit_and_score(clf_multi_nb, X_train, y_train, X_adjs.columns, 'naive-bayes')\n",
      "\n",
      "print 'Using testing set'\n",
      "fit_and_score(clf_multi_nb, X_test, y_test, X_adjs.columns, 'naive-bayes')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "SCORE:  0.901251393879\n",
        "AUC:  0.502431233668\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "        adjective      coef   p-value\n",
        "163         other  0.005634  0.000050\n",
        "26           cant  0.009299  0.000081\n",
        "73           epic  0.000679  0.000087\n",
        "210      spacious  0.000679  0.000087\n",
        "250        yellow  0.000679  0.000087\n",
        "87      flavorful  0.001290  0.000174\n",
        "5      affordable  0.001901  0.000261\n",
        "96        general  0.002579  0.001122\n",
        "209         solid  0.001968  0.001918\n",
        "122         irish  0.001358  0.003553\n",
        "215         steak  0.006109  0.004008\n",
        "95         garlic  0.004277  0.004226\n",
        "46           cozy  0.001833  0.006104\n",
        "115           hot  0.012015  0.006348\n",
        "227      terrible  0.002647  0.007124\n",
        "158         olive  0.000747  0.008315\n",
        "237  unbelievable  0.000747  0.008315\n",
        "38          clean  0.004548  0.008604\n",
        "138          long  0.005430  0.009899\n",
        "84          first  0.004209  0.011209\n",
        "56           dish  0.002987  0.013767\n",
        "140         magic  0.000611  0.014504\n",
        "151       natural  0.000611  0.014504\n",
        "191           sad  0.000611  0.014504\n",
        "216       stellar  0.000611  0.014504\n",
        "119        indian  0.001765  0.020247\n",
        "157           old  0.004005  0.020831\n",
        "223         sweet  0.007738  0.023556\n",
        "118    incredible  0.002104  0.025013\n",
        "169      personal  0.001154  0.029040\n",
        "170    phenomenal  0.001154  0.029040\n",
        "228      terrific  0.001154  0.029040\n",
        "107       grilled  0.005974  0.031371\n",
        "81            fat  0.000815  0.033534\n",
        "127           kid  0.000815  0.033534\n",
        "162      original  0.000815  0.033534\n",
        "178        public  0.000815  0.033534\n",
        "239         usual  0.000815  0.033534\n",
        "54      different  0.002308  0.033618\n",
        "82       favorite  0.006652  0.038926\n",
        "\n",
        "\n",
        "Using testing set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "SCORE:  0.892976588629\n",
        "AUC:  0.518394648829\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "       adjective      coef   p-value\n",
        "14         baked  0.003368  0.000000\n",
        "56          dish  0.003368  0.000000\n",
        "83           few  0.003368  0.000000\n",
        "93          full  0.003368  0.000000\n",
        "120  inexpensive  0.001783  0.000000\n",
        "35       chinese  0.009907  0.002318\n",
        "179        quick  0.004755  0.004851\n",
        "114     horrible  0.003566  0.006621\n",
        "130        large  0.003566  0.006621\n",
        "154         next  0.003566  0.006621\n",
        "124          ive  0.010105  0.009112\n",
        "68      eggplant  0.001981  0.012537\n",
        "113         high  0.001981  0.012537\n",
        "118   incredible  0.001981  0.012537\n",
        "146     mediocre  0.001981  0.012537\n",
        "149         much  0.006142  0.014883\n",
        "10     available  0.001585  0.015660\n",
        "25          busy  0.001585  0.015660\n",
        "47         crazy  0.001585  0.015660\n",
        "104       greasy  0.001585  0.015660\n",
        "133        later  0.001585  0.015660\n",
        "199        short  0.001585  0.015660\n",
        "163        other  0.004557  0.020173\n",
        "116         huge  0.003765  0.025169\n",
        "254        yummy  0.008916  0.040728\n",
        "5     affordable  0.002179  0.045608\n",
        "23        bubble  0.002179  0.045608\n",
        "54     different  0.002179  0.045608\n",
        "131         last  0.002179  0.045608\n",
        "51        decent  0.007331  0.049499\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_linear = linear_model.LinearRegression()\n",
      "\n",
      "print 'Using training set'\n",
      "fit_and_score(clf_linear, X_train, y_train, X_adjs.columns, 'linear')\n",
      "\n",
      "print 'Using testing set'\n",
      "fit_and_score(clf_linear, X_test, y_test, X_adjs.columns, 'linear')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
        "SCORE:  0.0306220058522\n",
        "AUC:  0.65453453789\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "        adjective    coef   p-value\n",
        "163         other -0.0005  0.000050\n",
        "26           cant  0.0110  0.000081\n",
        "73           epic  0.0196  0.000087\n",
        "250        yellow  0.0143  0.000087\n",
        "210      spacious  0.0123  0.000087\n",
        "87      flavorful  0.0218  0.000174\n",
        "5      affordable  0.0061  0.000261\n",
        "96        general  0.0292  0.001122\n",
        "209         solid  0.0167  0.001918\n",
        "122         irish -0.0082  0.003553\n",
        "215         steak -0.0008  0.004008\n",
        "95         garlic -0.0077  0.004226\n",
        "46           cozy -0.0200  0.006104\n",
        "115           hot  0.0042  0.006348\n",
        "227      terrible -0.0021  0.007124\n",
        "158         olive  0.0464  0.008315\n",
        "237  unbelievable  0.0079  0.008315\n",
        "38          clean  0.0037  0.008604\n",
        "138          long -0.0036  0.009899\n",
        "84          first -0.0020  0.011209\n",
        "56           dish  0.0091  0.013767\n",
        "151       natural  0.0361  0.014504\n",
        "191           sad  0.0085  0.014504\n",
        "216       stellar -0.0123  0.014504\n",
        "140         magic -0.0354  0.014504\n",
        "119        indian -0.0069  0.020247\n",
        "157           old -0.0030  0.020831\n",
        "223         sweet  0.0028  0.023556\n",
        "118    incredible  0.0127  0.025013\n",
        "228      terrific  0.0222  0.029040\n",
        "169      personal -0.0176  0.029040\n",
        "170    phenomenal -0.0186  0.029040\n",
        "107       grilled  0.0041  0.031371\n",
        "239         usual  0.0336  0.033534\n",
        "127           kid  0.0176  0.033534\n",
        "162      original  0.0168  0.033534\n",
        "81            fat  0.0163  0.033534\n",
        "178        public  0.0145  0.033534\n",
        "54      different -0.0040  0.033618\n",
        "82       favorite  0.0016  0.038926\n",
        "\n",
        "\n",
        "Using testing set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
        "SCORE:  0.0845980257986\n",
        "AUC:  0.722498909408\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "       adjective    coef   p-value\n",
        "56          dish  0.0036  0.000000\n",
        "83           few  0.0002  0.000000\n",
        "14         baked -0.0032  0.000000\n",
        "93          full -0.0215  0.000000\n",
        "120  inexpensive -0.0343  0.000000\n",
        "35       chinese  0.0094  0.002318\n",
        "179        quick  0.0308  0.004851\n",
        "130        large  0.0649  0.006621\n",
        "154         next -0.0025  0.006621\n",
        "114     horrible -0.0026  0.006621\n",
        "124          ive  0.0057  0.009112\n",
        "68      eggplant  0.0817  0.012537\n",
        "146     mediocre  0.0435  0.012537\n",
        "118   incredible  0.0099  0.012537\n",
        "113         high -0.0443  0.012537\n",
        "149         much  0.0259  0.014883\n",
        "47         crazy  0.0258  0.015660\n",
        "10     available -0.0074  0.015660\n",
        "133        later -0.0150  0.015660\n",
        "199        short -0.0151  0.015660\n",
        "25          busy -0.0280  0.015660\n",
        "104       greasy -0.0347  0.015660\n",
        "163        other -0.0045  0.020173\n",
        "116         huge  0.0033  0.025169\n",
        "254        yummy -0.0219  0.040728\n",
        "131         last  0.0762  0.045608\n",
        "5     affordable  0.0614  0.045608\n",
        "54     different  0.0041  0.045608\n",
        "23        bubble -0.0109  0.045608\n",
        "51        decent -0.0356  0.049499\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_logistic = linear_model.LogisticRegression()\n",
      "\n",
      "print 'Using training set'\n",
      "fit_and_score(clf_logistic, X_train, y_train, X_adjs.columns, 'logistic')\n",
      "\n",
      "print 'Using testing set'\n",
      "fit_and_score(clf_logistic, X_test, y_test, X_adjs.columns, 'logistic')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
        "SCORE:  0.901003593111\n",
        "AUC:  0.500625\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "        adjective      coef   p-value\n",
        "163         other  1.057386  0.000050\n",
        "26           cant  1.184831  0.000081\n",
        "73           epic  1.098779  0.000087\n",
        "250        yellow  1.097571  0.000087\n",
        "210      spacious  1.052954  0.000087\n",
        "87      flavorful  1.143622  0.000174\n",
        "5      affordable  1.044564  0.000261\n",
        "96        general  1.248696  0.001122\n",
        "209         solid  1.152807  0.001918\n",
        "122         irish  0.962232  0.003553\n",
        "215         steak  0.986887  0.004008\n",
        "95         garlic  0.938474  0.004226\n",
        "46           cozy  0.893776  0.006104\n",
        "115           hot  1.031176  0.006348\n",
        "227      terrible  1.015316  0.007124\n",
        "158         olive  1.137463  0.008315\n",
        "237  unbelievable  1.029219  0.008315\n",
        "38          clean  1.051376  0.008604\n",
        "138          long  0.981474  0.009899\n",
        "84          first  0.957337  0.011209\n",
        "56           dish  1.086107  0.013767\n",
        "151       natural  1.067479  0.014504\n",
        "191           sad  1.002704  0.014504\n",
        "216       stellar  0.941576  0.014504\n",
        "140         magic  0.860020  0.014504\n",
        "119        indian  0.969282  0.020247\n",
        "157           old  0.971514  0.020831\n",
        "223         sweet  1.015316  0.023556\n",
        "118    incredible  1.085239  0.025013\n",
        "228      terrific  1.048332  0.029040\n",
        "169      personal  0.903481  0.029040\n",
        "170    phenomenal  0.886300  0.029040\n",
        "107       grilled  1.022244  0.031371\n",
        "239         usual  1.144651  0.033534\n",
        "81            fat  1.101199  0.033534\n",
        "162      original  1.086650  0.033534\n",
        "127           kid  1.086107  0.033534\n",
        "178        public  1.072186  0.033534\n",
        "54      different  0.943933  0.033618\n",
        "82       favorite  1.031589  0.038926\n",
        "\n",
        "\n",
        "Using testing set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
        "SCORE:  0.889260497956\n",
        "AUC:  0.501672240803\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "       adjective      coef   p-value\n",
        "83           few  1.087846  0.000000\n",
        "56          dish  1.057492  0.000000\n",
        "14         baked  1.003807  0.000000\n",
        "93          full  0.949709  0.000000\n",
        "120  inexpensive  0.940353  0.000000\n",
        "35       chinese  1.041748  0.002318\n",
        "179        quick  1.140424  0.004851\n",
        "130        large  1.178096  0.006621\n",
        "114     horrible  1.037589  0.006621\n",
        "154         next  1.000400  0.006621\n",
        "124          ive  1.048856  0.009112\n",
        "68      eggplant  1.135190  0.012537\n",
        "146     mediocre  1.129189  0.012537\n",
        "118   incredible  1.043625  0.012537\n",
        "113         high  0.928486  0.012537\n",
        "149         much  1.092644  0.014883\n",
        "47         crazy  1.001501  0.015660\n",
        "10     available  0.950849  0.015660\n",
        "25          busy  0.933047  0.015660\n",
        "199        short  0.905924  0.015660\n",
        "104       greasy  0.889763  0.015660\n",
        "133        later  0.881086  0.015660\n",
        "163        other  0.921917  0.020173\n",
        "116         huge  1.089262  0.025169\n",
        "254        yummy  0.872145  0.040728\n",
        "131         last  1.405229  0.045608\n",
        "5     affordable  1.232692  0.045608\n",
        "54     different  1.051797  0.045608\n",
        "23        bubble  1.037382  0.045608\n",
        "51        decent  0.822588  0.049499\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Predicting Grade \"C\" restaurants"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = tips_adj_df['grade_C']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = train_test_split(X_adjs, y, random_state=8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_multi_nb = naive_bayes.MultinomialNB()\n",
      "\n",
      "print 'Using training set'\n",
      "fit_and_score(clf_multi_nb, X_train, y_train, X_adjs.columns, 'naive-bayes')\n",
      "\n",
      "print 'Using testing set'\n",
      "fit_and_score(clf_multi_nb, X_test, y_test, X_adjs.columns, 'naive-bayes')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "SCORE:  0.987981662743\n",
        "AUC:  0.499937304075\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "      adjective      coef   p-value\n",
        "53       delish  0.004338  0.000169\n",
        "74    excellent  0.006507  0.000341\n",
        "52    delicious  0.015184  0.001837\n",
        "106       green  0.004338  0.003959\n",
        "249       wrong  0.004338  0.005746\n",
        "22    breakfast  0.006507  0.006458\n",
        "0    20500daily  0.002169  0.012036\n",
        "24        bushy  0.002169  0.012036\n",
        "117      iconic  0.002169  0.012036\n",
        "176   priceless  0.002169  0.012036\n",
        "253        yous  0.002169  0.012036\n",
        "153         new  0.008677  0.017581\n",
        "107     grilled  0.004338  0.018048\n",
        "102        good  0.036876  0.023534\n",
        "171         pic  0.002169  0.024075\n",
        "192       salad  0.008677  0.029159\n",
        "19          big  0.004338  0.034574\n",
        "67    efficient  0.002169  0.036117\n",
        "99    ginormous  0.002169  0.036117\n",
        "196    separate  0.002169  0.036117\n",
        "203       sized  0.002169  0.036117\n",
        "221    superior  0.002169  0.036117\n",
        "84        first  0.004338  0.046109\n",
        "188       royal  0.002169  0.048163\n",
        "\n",
        "\n",
        "Using testing set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "SCORE:  0.990338164251\n",
        "AUC:  0.5\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "         adjective      coef   p-value\n",
        "43   complimentary  0.003333  0.009753\n",
        "44       congested  0.003333  0.009753\n",
        "48        creative  0.003333  0.009753\n",
        "67       efficient  0.003333  0.009753\n",
        "69           empty  0.003333  0.009753\n",
        "117         iconic  0.003333  0.009753\n",
        "151        natural  0.003333  0.009753\n",
        "169       personal  0.003333  0.009753\n",
        "171            pic  0.003333  0.009753\n",
        "188          royal  0.003333  0.009753\n",
        "190        russian  0.003333  0.009753\n",
        "193       saltfish  0.003333  0.009753\n",
        "196       separate  0.003333  0.009753\n",
        "198        several  0.003333  0.009753\n",
        "236     unbeatable  0.003333  0.009753\n",
        "243           weak  0.003333  0.009753\n",
        "2           actual  0.003333  0.019512\n",
        "31         certain  0.003333  0.019512\n",
        "39           clear  0.003333  0.019512\n",
        "103       gorgeous  0.003333  0.019512\n",
        "140          magic  0.003333  0.019512\n",
        "144        massive  0.003333  0.019512\n",
        "161        organic  0.003333  0.019512\n",
        "187       rosemary  0.003333  0.019512\n",
        "191            sad  0.003333  0.019512\n",
        "201        similar  0.003333  0.019512\n",
        "203          sized  0.003333  0.019512\n",
        "210       spacious  0.003333  0.019512\n",
        "228       terrific  0.003333  0.019512\n",
        "229           thin  0.003333  0.019512\n",
        "..             ...       ...       ...\n",
        "119         indian  0.003333  0.039054\n",
        "127            kid  0.003333  0.039054\n",
        "145           mean  0.003333  0.039054\n",
        "152         nearby  0.003333  0.039054\n",
        "174           poor  0.003333  0.039054\n",
        "207         social  0.003333  0.039054\n",
        "214       standard  0.003333  0.039054\n",
        "224          swiss  0.003333  0.039054\n",
        "234        turkish  0.003333  0.039054\n",
        "155           nice  0.006667  0.041094\n",
        "16            bean  0.003333  0.048836\n",
        "64            east  0.003333  0.048836\n",
        "70         english  0.003333  0.048836\n",
        "72          entire  0.003333  0.048836\n",
        "73            epic  0.003333  0.048836\n",
        "112        helpful  0.003333  0.048836\n",
        "121    interesting  0.003333  0.048836\n",
        "129         korean  0.003333  0.048836\n",
        "134          light  0.003333  0.048836\n",
        "139            low  0.003333  0.048836\n",
        "148         modern  0.003333  0.048836\n",
        "156         normal  0.003333  0.048836\n",
        "177        private  0.003333  0.048836\n",
        "186     ridiculous  0.003333  0.048836\n",
        "197        serious  0.003333  0.048836\n",
        "200       sicilian  0.003333  0.048836\n",
        "216        stellar  0.003333  0.048836\n",
        "218         strong  0.003333  0.048836\n",
        "235        typical  0.003333  0.048836\n",
        "244          weird  0.003333  0.048836\n",
        "\n",
        "[83 rows x 3 columns]\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_linear = linear_model.LinearRegression()\n",
      "\n",
      "print 'Using training set'\n",
      "fit_and_score(clf_linear, X_train, y_train, X_adjs.columns, 'linear')\n",
      "\n",
      "print 'Using testing set'\n",
      "fit_and_score(clf_linear, X_test, y_test, X_adjs.columns, 'linear')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
        "SCORE:  0.0265341264606\n",
        "AUC:  0.813025731452\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "      adjective    coef   p-value\n",
        "53       delish  0.0025  0.000169\n",
        "74    excellent  0.0006  0.000341\n",
        "52    delicious  0.0004  0.001837\n",
        "106       green  0.0031  0.003959\n",
        "249       wrong  0.0009  0.005746\n",
        "22    breakfast -0.0010  0.006458\n",
        "117      iconic  0.0080  0.012036\n",
        "24        bushy -0.0028  0.012036\n",
        "0    20500daily -0.0091  0.012036\n",
        "176   priceless -0.0091  0.012036\n",
        "253        yous -0.0103  0.012036\n",
        "153         new  0.0012  0.017581\n",
        "107     grilled  0.0003  0.018048\n",
        "102        good  0.0016  0.023534\n",
        "171         pic -0.0208  0.024075\n",
        "192       salad -0.0021  0.029159\n",
        "19          big  0.0016  0.034574\n",
        "203       sized  0.0032  0.036117\n",
        "196    separate  0.0028  0.036117\n",
        "221    superior -0.0084  0.036117\n",
        "67    efficient -0.0124  0.036117\n",
        "99    ginormous -0.0202  0.036117\n",
        "84        first  0.0046  0.046109\n",
        "188       royal -0.0046  0.048163\n",
        "\n",
        "\n",
        "Using testing set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
        "SCORE:  0.0506823204213\n",
        "AUC:  0.890532544379\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "         adjective    coef   p-value\n",
        "196       separate  0.0067  0.009753\n",
        "44       congested  0.0036  0.009753\n",
        "190        russian  0.0001  0.009753\n",
        "69           empty -0.0023  0.009753\n",
        "169       personal -0.0058  0.009753\n",
        "193       saltfish -0.0058  0.009753\n",
        "236     unbeatable -0.0058  0.009753\n",
        "151        natural -0.0134  0.009753\n",
        "171            pic -0.0134  0.009753\n",
        "188          royal -0.0134  0.009753\n",
        "198        several -0.0134  0.009753\n",
        "243           weak -0.0134  0.009753\n",
        "67       efficient -0.0188  0.009753\n",
        "43   complimentary -0.0223  0.009753\n",
        "117         iconic -0.1083  0.009753\n",
        "48        creative -0.1467  0.009753\n",
        "187       rosemary  0.0239  0.019512\n",
        "31         certain  0.0054  0.019512\n",
        "229           thin  0.0029  0.019512\n",
        "2           actual  0.0020  0.019512\n",
        "161        organic  0.0014  0.019512\n",
        "103       gorgeous  0.0005  0.019512\n",
        "140          magic -0.0017  0.019512\n",
        "252          young -0.0037  0.019512\n",
        "237   unbelievable -0.0072  0.019512\n",
        "39           clear -0.0094  0.019512\n",
        "144        massive -0.0104  0.019512\n",
        "191            sad -0.0110  0.019512\n",
        "203          sized -0.0134  0.019512\n",
        "228       terrific -0.0141  0.019512\n",
        "..             ...     ...       ...\n",
        "30         central -0.0045  0.039054\n",
        "207         social -0.0053  0.039054\n",
        "145           mean -0.0055  0.039054\n",
        "152         nearby -0.0080  0.039054\n",
        "63           earth -0.0093  0.039054\n",
        "1           accept -0.0098  0.039054\n",
        "119         indian -0.0108  0.039054\n",
        "28       caribbean -0.0183  0.039054\n",
        "174           poor -0.0398  0.039054\n",
        "155           nice -0.0036  0.041094\n",
        "235        typical  0.0033  0.048836\n",
        "156         normal -0.0012  0.048836\n",
        "200       sicilian -0.0020  0.048836\n",
        "134          light -0.0047  0.048836\n",
        "216        stellar -0.0048  0.048836\n",
        "244          weird -0.0048  0.048836\n",
        "218         strong -0.0067  0.048836\n",
        "112        helpful -0.0073  0.048836\n",
        "139            low -0.0085  0.048836\n",
        "186     ridiculous -0.0090  0.048836\n",
        "197        serious -0.0122  0.048836\n",
        "73            epic -0.0125  0.048836\n",
        "16            bean -0.0171  0.048836\n",
        "70         english -0.0192  0.048836\n",
        "148         modern -0.0200  0.048836\n",
        "121    interesting -0.0213  0.048836\n",
        "64            east -0.0218  0.048836\n",
        "129         korean -0.0268  0.048836\n",
        "177        private -0.0272  0.048836\n",
        "72          entire -0.0282  0.048836\n",
        "\n",
        "[83 rows x 3 columns]\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_logistic = linear_model.LogisticRegression()\n",
      "\n",
      "print 'Using training set'\n",
      "fit_and_score(clf_logistic, X_train, y_train, X_adjs.columns, 'logistic')\n",
      "\n",
      "print 'Using testing set'\n",
      "fit_and_score(clf_logistic, X_test, y_test, X_adjs.columns, 'logistic')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
        "SCORE:  0.988105563127\n",
        "AUC:  0.5\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "      adjective      coef   p-value\n",
        "53       delish  1.041227  0.000169\n",
        "74    excellent  0.962809  0.000341\n",
        "52    delicious  0.975505  0.001837\n",
        "106       green  1.059291  0.003959\n",
        "249       wrong  0.952657  0.005746\n",
        "22    breakfast  0.981376  0.006458\n",
        "24        bushy  0.992131  0.012036\n",
        "117      iconic  0.990644  0.012036\n",
        "0    20500daily  0.989060  0.012036\n",
        "176   priceless  0.989060  0.012036\n",
        "253        yous  0.988665  0.012036\n",
        "153         new  1.037486  0.017581\n",
        "107     grilled  1.029425  0.018048\n",
        "102        good  1.020814  0.023534\n",
        "171         pic  0.962424  0.024075\n",
        "192       salad  0.970543  0.029159\n",
        "19          big  0.932674  0.034574\n",
        "203       sized  0.980787  0.036117\n",
        "196    separate  0.975017  0.036117\n",
        "221    superior  0.968507  0.036117\n",
        "67    efficient  0.962713  0.036117\n",
        "99    ginormous  0.961655  0.036117\n",
        "84        first  1.126257  0.046109\n",
        "188       royal  0.962617  0.048163\n",
        "\n",
        "\n",
        "Using testing set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
        "SCORE:  0.990338164251\n",
        "AUC:  0.5\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "         adjective      coef   p-value\n",
        "44       congested  0.995012  0.009753\n",
        "190        russian  0.993422  0.009753\n",
        "196       separate  0.992727  0.009753\n",
        "169       personal  0.992429  0.009753\n",
        "193       saltfish  0.992429  0.009753\n",
        "236     unbeatable  0.992429  0.009753\n",
        "69           empty  0.989258  0.009753\n",
        "67       efficient  0.987578  0.009753\n",
        "151        natural  0.987282  0.009753\n",
        "171            pic  0.987282  0.009753\n",
        "188          royal  0.987282  0.009753\n",
        "198        several  0.987282  0.009753\n",
        "243           weak  0.987282  0.009753\n",
        "43   complimentary  0.982554  0.009753\n",
        "117         iconic  0.972291  0.009753\n",
        "48        creative  0.939789  0.009753\n",
        "187       rosemary  0.995610  0.019512\n",
        "31         certain  0.988566  0.019512\n",
        "2           actual  0.985802  0.019512\n",
        "140          magic  0.982947  0.019512\n",
        "252          young  0.982849  0.019512\n",
        "103       gorgeous  0.981474  0.019512\n",
        "210       spacious  0.980983  0.019512\n",
        "161        organic  0.980591  0.019512\n",
        "229           thin  0.980297  0.019512\n",
        "228       terrific  0.978142  0.019512\n",
        "237   unbelievable  0.977556  0.019512\n",
        "191            sad  0.976774  0.019512\n",
        "39           clear  0.974822  0.019512\n",
        "203          sized  0.973556  0.019512\n",
        "..             ...       ...       ...\n",
        "234        turkish  0.966475  0.039054\n",
        "207         social  0.963580  0.039054\n",
        "28       caribbean  0.961751  0.039054\n",
        "30         central  0.960213  0.039054\n",
        "63           earth  0.955902  0.039054\n",
        "152         nearby  0.953420  0.039054\n",
        "119         indian  0.953324  0.039054\n",
        "1           accept  0.948759  0.039054\n",
        "174           poor  0.926075  0.039054\n",
        "155           nice  0.822506  0.041094\n",
        "235        typical  0.966765  0.048836\n",
        "216        stellar  0.962520  0.048836\n",
        "139            low  0.958199  0.048836\n",
        "244          weird  0.956571  0.048836\n",
        "218         strong  0.955997  0.048836\n",
        "148         modern  0.955137  0.048836\n",
        "112        helpful  0.954660  0.048836\n",
        "134          light  0.954183  0.048836\n",
        "200       sicilian  0.953134  0.048836\n",
        "186     ridiculous  0.952943  0.048836\n",
        "70         english  0.947716  0.048836\n",
        "72          entire  0.946580  0.048836\n",
        "156         normal  0.945728  0.048836\n",
        "16            bean  0.944216  0.048836\n",
        "64            east  0.943556  0.048836\n",
        "177        private  0.942707  0.048836\n",
        "73            epic  0.941576  0.048836\n",
        "129         korean  0.941294  0.048836\n",
        "197        serious  0.939977  0.048836\n",
        "121    interesting  0.936599  0.048836\n",
        "\n",
        "[83 rows x 3 columns]\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=5)\n",
      "\n",
      "print 'Using training set'\n",
      "clf_tree = clf_tree.fit(X_train, y_train)\n",
      "score = clf_tree.score(X_train, y_train)\n",
      "y_pred = clf_tree.predict(X_train)\n",
      "\n",
      "print \"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_train,y_pred)), \"\\n\"\n",
      "print \"Classification report\"\n",
      "print metrics.classification_report(y_train, y_pred), \"\\n\"\n",
      "        \n",
      "print \"Confusion matrix\"\n",
      "print metrics.confusion_matrix(y_train, y_pred), \"\\n\"\n",
      "\n",
      "clf_tree_2 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=5)\n",
      "\n",
      "print 'Using testing set'\n",
      "clf_tree_2 = clf_tree_2.fit(X_test, y_test)\n",
      "score = clf_tree_2.score(X_test, y_test)\n",
      "y_pred = clf_tree_2.predict(X_test)\n",
      "\n",
      "print \"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_test,y_pred)), \"\\n\"\n",
      "print \"Classification report\"\n",
      "print metrics.classification_report(y_test, y_pred), \"\\n\"\n",
      "        \n",
      "print \"Confusion matrix\"\n",
      "print metrics.confusion_matrix(y_test, y_pred), \"\\n\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "Accuracy:0.988"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n",
        "\n",
        "Classification report\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.99      1.00      0.99      7975\n",
        "        1.0       0.00      0.00      0.00        96\n",
        "\n",
        "avg / total       0.98      0.99      0.98      8071\n",
        "\n",
        "\n",
        "Confusion matrix\n",
        "[[7975    0]\n",
        " [  96    0]] \n",
        "\n",
        "Using testing set\n",
        "Accuracy:0.990 \n",
        "\n",
        "Classification report\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.99      1.00      1.00      2665\n",
        "        1.0       0.00      0.00      0.00        26\n",
        "\n",
        "avg / total       0.98      0.99      0.99      2691\n",
        "\n",
        "\n",
        "Confusion matrix\n",
        "[[2665    0]\n",
        " [  26    0]] \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = tips_adj_df[tips_adj_df['foursquare_price_tier'] == 1]['grade_C']\n",
      "cheap_df = tips_adj_df[tips_adj_df['foursquare_price_tier'] == 1]\n",
      "X_adjs = cheap_df.ix[:, 34:-1]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_adjs, y, random_state=8)\n",
      "\n",
      "clf_multi_nb = naive_bayes.MultinomialNB()\n",
      "\n",
      "print 'Using training set'\n",
      "fit_and_score(clf_multi_nb, X_train, y_train, X_adjs.columns, 'naive-bayes')\n",
      "\n",
      "print 'Using testing set'\n",
      "fit_and_score(clf_multi_nb, X_test, y_test, X_adjs.columns, 'naive-bayes')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using training set\n",
        "MODEL:  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "SCORE:  0.990886998785\n",
        "AUC:  0.5\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "       adjective      coef   p-value\n",
        "10     available  0.003484  0.009191\n",
        "11       average  0.003484  0.009191\n",
        "24         bushy  0.003484  0.009191\n",
        "31       certain  0.003484  0.009191\n",
        "67     efficient  0.003484  0.009191\n",
        "73          epic  0.003484  0.009191\n",
        "86          flat  0.003484  0.009191\n",
        "111        heavy  0.003484  0.009191\n",
        "126          key  0.003484  0.009191\n",
        "148       modern  0.003484  0.009191\n",
        "151      natural  0.003484  0.009191\n",
        "161      organic  0.003484  0.009191\n",
        "171          pic  0.003484  0.009191\n",
        "172     pleasant  0.003484  0.009191\n",
        "186   ridiculous  0.003484  0.009191\n",
        "190      russian  0.003484  0.009191\n",
        "193     saltfish  0.003484  0.009191\n",
        "200     sicilian  0.003484  0.009191\n",
        "201      similar  0.003484  0.009191\n",
        "203        sized  0.003484  0.009191\n",
        "216      stellar  0.003484  0.009191\n",
        "234      turkish  0.003484  0.009191\n",
        "236   unbeatable  0.003484  0.009191\n",
        "238       unique  0.003484  0.009191\n",
        "5     affordable  0.003484  0.018394\n",
        "7          asian  0.003484  0.018394\n",
        "8      attentive  0.003484  0.018394\n",
        "15         basic  0.003484  0.018394\n",
        "30       central  0.003484  0.018394\n",
        "48      creative  0.003484  0.018394\n",
        "..           ...       ...       ...\n",
        "224        swiss  0.003484  0.027608\n",
        "230         tiny  0.003484  0.027608\n",
        "233         true  0.003484  0.027608\n",
        "252        young  0.003484  0.027608\n",
        "102         good  0.010453  0.028718\n",
        "12         awful  0.003484  0.036833\n",
        "18     beautiful  0.003484  0.036833\n",
        "47         crazy  0.003484  0.036833\n",
        "57          dive  0.003484  0.036833\n",
        "72        entire  0.003484  0.036833\n",
        "101         goat  0.003484  0.036833\n",
        "104       greasy  0.003484  0.036833\n",
        "113         high  0.003484  0.036833\n",
        "158        olive  0.003484  0.036833\n",
        "165      overall  0.003484  0.036833\n",
        "166          own  0.003484  0.036833\n",
        "202       simple  0.003484  0.036833\n",
        "211      spanish  0.003484  0.036833\n",
        "217     straight  0.003484  0.036833\n",
        "219         such  0.003484  0.036833\n",
        "232  traditional  0.003484  0.036833\n",
        "16          bean  0.003484  0.046069\n",
        "40         close  0.003484  0.046069\n",
        "55         dirty  0.003484  0.046069\n",
        "65          easy  0.003484  0.046069\n",
        "81           fat  0.003484  0.046069\n",
        "87     flavorful  0.003484  0.046069\n",
        "183   reasonable  0.003484  0.046069\n",
        "235      typical  0.003484  0.046069\n",
        "244        weird  0.003484  0.046069\n",
        "\n",
        "[116 rows x 3 columns]\n",
        "\n",
        "\n",
        "Using testing set\n",
        "MODEL: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "SCORE:  0.992714025501\n",
        "AUC:  0.5\n",
        "\n",
        "\n",
        "TOP PREDICTORS (p-value < 0.05):\n",
        "      adjective      coef   p-value\n",
        "3     addictive  0.003817  0.007326\n",
        "5    affordable  0.003817  0.007326\n",
        "6      american  0.003817  0.007326\n",
        "7         asian  0.003817  0.007326\n",
        "8     attentive  0.003817  0.007326\n",
        "10    available  0.003817  0.007326\n",
        "11      average  0.003817  0.007326\n",
        "14        baked  0.003817  0.007326\n",
        "23       bubble  0.003817  0.007326\n",
        "29       casual  0.003817  0.007326\n",
        "31      certain  0.003817  0.007326\n",
        "46         cozy  0.003817  0.007326\n",
        "55        dirty  0.003817  0.007326\n",
        "56         dish  0.003817  0.007326\n",
        "61          dry  0.003817  0.007326\n",
        "65         easy  0.003817  0.007326\n",
        "68     eggplant  0.003817  0.007326\n",
        "69        empty  0.003817  0.007326\n",
        "73         epic  0.003817  0.007326\n",
        "75    expensive  0.003817  0.007326\n",
        "77     fabulous  0.003817  0.007326\n",
        "86         flat  0.003817  0.007326\n",
        "87    flavorful  0.003817  0.007326\n",
        "89       french  0.003817  0.007326\n",
        "93         full  0.003817  0.007326\n",
        "94        funny  0.003817  0.007326\n",
        "97     generous  0.003817  0.007326\n",
        "99    ginormous  0.003817  0.007326\n",
        "101        goat  0.003817  0.007326\n",
        "112     helpful  0.003817  0.007326\n",
        "..          ...       ...       ...\n",
        "130       large  0.003817  0.029467\n",
        "132        late  0.003817  0.029467\n",
        "152      nearby  0.003817  0.029467\n",
        "157         old  0.003817  0.029467\n",
        "181       ready  0.003817  0.029467\n",
        "184         red  0.003817  0.029467\n",
        "12        awful  0.003817  0.036902\n",
        "20        black  0.003817  0.036902\n",
        "25         busy  0.003817  0.036902\n",
        "49         cute  0.003817  0.036902\n",
        "51       decent  0.003817  0.036902\n",
        "53       delish  0.003817  0.036902\n",
        "83          few  0.003817  0.036902\n",
        "85         fish  0.003817  0.036902\n",
        "96      general  0.003817  0.036902\n",
        "182        real  0.003817  0.036902\n",
        "215       steak  0.003817  0.036902\n",
        "242        want  0.003817  0.036902\n",
        "13          bad  0.003817  0.044365\n",
        "38        clean  0.003817  0.044365\n",
        "74    excellent  0.003817  0.044365\n",
        "80         fast  0.003817  0.044365\n",
        "100        give  0.003817  0.044365\n",
        "108       happy  0.003817  0.044365\n",
        "116        huge  0.003817  0.044365\n",
        "168     perfect  0.003817  0.044365\n",
        "185     regular  0.003817  0.044365\n",
        "205       small  0.003817  0.044365\n",
        "209       solid  0.003817  0.044365\n",
        "251       youll  0.003817  0.044365\n",
        "\n",
        "[144 rows x 3 columns]\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tips_adj_df.to_pickle('./dumps/tips_complete_features.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    }
   ],
   "metadata": {}
  }
 ]
}